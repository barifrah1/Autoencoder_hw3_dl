{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.0 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "764e53dc5e3cc7b54d77b51f4fb7ab4789ad4d184d9b79883b073a8f85e8cc44"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDRrkLUA2j0G"
      },
      "source": [
        "# Deep Learning Course - HW3 - Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOhfNqxi21_Q"
      },
      "source": [
        "# intial Process of the data\r\n",
        "We load the csv and we save in dictionary for each user,list of all the movies that he saw.\r\n",
        "We take one random item from each user to validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import random as random\n",
        "from numpy.random import rand\n",
        "import torch as torch\n",
        "from torch import nn,optim\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "USER_IND = 0\n",
        "ITEM_IND = 1\n",
        "INPUT_SIZE = 3706\n",
        "TRAIN_DATA_PATH = \"Train.csv\"\n",
        "TEST_DATA_RANDOM_PATH = \"RandomTest.csv\"\n",
        "TEST_DATA_POPULAR_PATH = \"PopularityTest.csv\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSMlYWS22eA6"
      },
      "source": [
        "def initialProcessData(path):\n",
        "    data = pd.read_csv(\n",
        "        path, sep=',', header=0).to_numpy()\n",
        "    train = {}\n",
        "    popularity = {}\n",
        "    for i in range(INPUT_SIZE):\n",
        "        popularity[i] = 0\n",
        "    # create training data\n",
        "    for row in data:\n",
        "        if row[USER_IND]-1 not in train.keys():\n",
        "            train[row[USER_IND]-1] = []\n",
        "        train[row[USER_IND]-1].append(row[ITEM_IND]-1)\n",
        "        popularity[row[ITEM_IND]-1] += 1\n",
        "    validation = {}\n",
        "    # create validation data\n",
        "    for user in train.keys():\n",
        "        if(len(train[user]) > 1):\n",
        "            validation_item = random.choice(train[user])\n",
        "            train[user].remove(validation_item)\n",
        "            validation[user] = [validation_item]\n",
        "    return train, validation, popularity"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRr9oImw4fw1"
      },
      "source": [
        "    d1, validation_data, popularity = initialProcessData(\n",
        "        TRAIN_DATA_PATH)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BzyPBJm40xx"
      },
      "source": [
        "# Dataloader class\r\n",
        "We used Dataloader_recsys class to manage all the data with functions like userSeenItems that returns list of items that the user have watched.\r\n",
        "userBinaryVector that returns binary vector at the size of the input with 1 in the positions where the movies he has already watched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQecNMJ441FM"
      },
      "source": [
        "class DataLoader_RecSys(Dataset):\n",
        "    def __init__(self, dataset, popularity):\n",
        "        self.dataset = dataset\n",
        "        self.popularity = popularity\n",
        "        self.popularity_prob = np.array(\n",
        "            list(self.popularity.values()))/sum(np.array(list(self.popularity.values())))\n",
        "        self.users = list(self.dataset.keys())\n",
        "        self.items = []\n",
        "        for user in self.users:\n",
        "            self.items = self.items + self.dataset[user]\n",
        "        self.items = Counter(self.items)\n",
        "        self.max_item_index = max(self.items)\n",
        "        self.max_user_index = max(self.users)\n",
        "\n",
        "    def userSeenItems(self, user):\n",
        "        return self.dataset[user]\n",
        "\n",
        "    def userBinaryVector(self, user):\n",
        "        userVector = np.zeros(self.max_item_index + 1)\n",
        "        userItems = self.userSeenItems(user)\n",
        "        for item in userItems:\n",
        "            userVector[item] = 1\n",
        "        return userVector\n",
        "\n",
        "    def userUnseenItems(self, user):\n",
        "        return list(set(self.items).difference(set(self.userSeenItems(user))))\n",
        "\n",
        "    def numOfUsers(self):\n",
        "        return self.max_user_index + 1\n",
        "\n",
        "    def numOfItems(self):\n",
        "        return self.max_item_index + 1\n",
        "\n",
        "    def drawUnseenItem(self, user):\n",
        "        return random.choice(self.userUnseenItems(user))\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        if(ind >= self.__len__()):\n",
        "            raise IndexError\n",
        "        userVec = self.userBinaryVector(ind)\n",
        "        return userVec\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_user_index + 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdZFBF586pFm"
      },
      "source": [
        "\n",
        "class AutoEncoderArgs:\n",
        "    num_epochs = 100\n",
        "    lr = 1e-4\n",
        "    weight_decay = 1e-7\n",
        "    input_size = 3706\n",
        "    hidden_size = 80\n",
        "    popularity_multiplyer = 200\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOmith5J6wVL"
      },
      "source": [
        "train_dataloader = DataLoader_RecSys(d1, popularity)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7DDgWSa82Ra"
      },
      "source": [
        "# AutoEncoder Class and training loop for randomDataset\r\n",
        "AutoEncoder class with 1 hidden layer for encoder and 1 hidden layer for decoder using sigmoid activation for non linearity.\r\n",
        "We saw that using dropout as regularization gets us better results on validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNbesNuE82sK"
      },
      "source": [
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, args=None):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.args = args\n",
        "        self.encoder = nn.Sequential(nn.Dropout(0.5),\n",
        "                                     nn.Linear(args.input_size,\n",
        "                                               args.hidden_size, bias=True),\n",
        "                                     nn.Sigmoid())\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(args.hidden_size, args.input_size, bias=True),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = torch.tensor(x).float()\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def infer(dataloader, validation, model):\n",
        "    accuracy = 0\n",
        "    counter = 0\n",
        "    index = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for userVec in dataloader:\n",
        "            userVec = torch.tensor(userVec).float()\n",
        "            output = model(userVec)\n",
        "            if(len(validation[index]) == 0):\n",
        "                index += 1\n",
        "                counter += 1\n",
        "                continue\n",
        "            validationUserSeenItem = validation[index][0]\n",
        "            itemDrawn = dataloader.drawUnseenItem(index)\n",
        "            while(itemDrawn == validationUserSeenItem):\n",
        "                itemDrawn = dataloader.drawUnseenItem(index)\n",
        "            if(output[validationUserSeenItem].item() > output[itemDrawn].item()):\n",
        "                accuracy += 1\n",
        "            index += 1\n",
        "    acc = accuracy/(dataloader.numOfUsers() - counter)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def training_loop(args,\n",
        "                  model,\n",
        "                  tr_dataloader=None,\n",
        "                  validation=None,\n",
        "                  criterion_func=nn.MSELoss,\n",
        "                  ):\n",
        "    accuracy_by_epoch = []\n",
        "    criterion = criterion_func()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr,\n",
        "                           weight_decay=args.weight_decay)\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "        model.train()\n",
        "        for userVec in tqdm(tr_dataloader):\n",
        "            userVec = torch.tensor(userVec).float()\n",
        "            # ===================forward=====================\n",
        "            output = model(userVec)\n",
        "            loss = criterion(output, userVec)\n",
        "            # ===================backward====================\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # ===================log========================\n",
        "        if epoch % 1 == 0:\n",
        "            currentAccuracy = infer(\n",
        "                tr_dataloader, validation, model)\n",
        "            accuracy_by_epoch.append(currentAccuracy)\n",
        "            print(\n",
        "                f\" epoch: { epoch+1} validation accuracy: {currentAccuracy}\")\n",
        "            if currentAccuracy > 0.932:\n",
        "                break\n",
        "    predict = pd.read_csv(\"RandomTest.csv\")\n",
        "    data = predict.values\n",
        "    for x in data:\n",
        "        user = x[0]-1\n",
        "        item1 = x[1]-1\n",
        "        item2 = x[2]-1\n",
        "        uservector = tr_dataloader.__getitem__(user)\n",
        "        uservector = torch.tensor(uservector).float()\n",
        "        with torch.no_grad():\n",
        "            output = model(uservector)\n",
        "            if output[item1] >= output[item2]:\n",
        "                x[3] = 0\n",
        "            if output[item1] < output[item2]:\n",
        "                x[3] = 1\n",
        "    df = pd.DataFrame(\n",
        "        data, columns=['UserID', 'Item1', 'Item2', 'bitClassification'])\n",
        "    df.to_csv(r'random_205592652_312425036.csv', index=False)\n",
        "\n",
        "    return accuracy_by_epoch\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNgEJxDN-YzU"
      },
      "source": [
        "Traing the model and print the randomDataset results to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WpN30JV-ZAz"
      },
      "source": [
        "\n",
        "args = AutoEncoderArgs()\n",
        "model = Autoencoder(args=args)\n",
        "training_loop(args,\n",
        "            model,\n",
        "            tr_dataloader=train_dataloader,\n",
        "            validation=validation_data,\n",
        "            criterion_func=nn.MSELoss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6040/6040 [01:38<00:00, 61.46it/s]\n",
            "  0%|          | 6/6040 [00:00<01:41, 59.26it/s] epoch: 1 validation accuracy: 0.8634105960264901\n",
            "100%|██████████| 6040/6040 [01:29<00:00, 67.52it/s]\n",
            "  0%|          | 6/6040 [00:00<01:46, 56.75it/s] epoch: 2 validation accuracy: 0.8534768211920529\n",
            " 67%|██████▋   | 4035/6040 [01:00<00:30, 66.26it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-70d9d5b2e2de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mtr_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             criterion_func=nn.MSELoss)\n\u001b[0m",
            "\u001b[1;32m<ipython-input-20-c37bfbfb4d96>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(args, model, tr_dataloader, validation, criterion_func)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# ===================backward====================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# ===================log========================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YyM128D-83R"
      },
      "source": [
        "# AutoEncoder and training loop for PopularityDataset \r\n",
        "We used AutoEncoderPopular class to find latent representation of user's binary vector.\r\n",
        "It's very to similar to the AutoEncoder class. The main change is the loss function. Each iteration we create a binary mask for the user where all the seen movies values are 1 and some of the unseen values are also 1. We draw the unseen movies with respect to the popularity distribution of the movies. We calculate the MSELoss on Autoencoder's output*mask to focus on the seen movies and unseen popular movies which are most likely to be in the Popularity test file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXfoczmK-9Ij"
      },
      "source": [
        "\n",
        "class AutoencoderPopular(nn.Module):\n",
        "    def __init__(self, args=None):\n",
        "        super(AutoencoderPopular, self).__init__()\n",
        "        self.args = args\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(args.input_size,\n",
        "                      args.hidden_size, bias=True),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(args.hidden_size, args.input_size, bias=True),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = torch.tensor(x).float()\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def infer(dataloader, validation, model):\n",
        "    accuracy = 0\n",
        "    counter = 0\n",
        "    index = 0\n",
        "    model.eval()\n",
        "    popProb = dataloader.popularity_prob\n",
        "    with torch.no_grad():\n",
        "        for userVec in tqdm(dataloader):\n",
        "            userPopProb = popProb.copy()\n",
        "            userItems = dataloader.userSeenItems(index)\n",
        "            for item in userItems:\n",
        "                userPopProb[item] = 1\n",
        "            userVec = torch.tensor(userVec).float()\n",
        "            output = model(userVec)\n",
        "            new_output = output\n",
        "            if(len(validation[index]) == 0):\n",
        "                index += 1\n",
        "                counter += 1\n",
        "                continue\n",
        "            validationUserSeenItem = validation[index][0]\n",
        "            # unseenPopularItemsList[index][0][epoch]\n",
        "            prob = userPopProb.copy()\n",
        "            for item in userItems:\n",
        "                prob[item] = 0\n",
        "            itemDrawn = random.choices(\n",
        "                range(model.args.input_size), weights=prob, k=1)\n",
        "            while(itemDrawn == validationUserSeenItem):\n",
        "                itemDrawn = random.choices(\n",
        "                    range(model.args.input_size), weights=prob, k=1)\n",
        "            if(new_output[validationUserSeenItem].item() > new_output[itemDrawn].item()):\n",
        "                accuracy += 1\n",
        "            index += 1\n",
        "    acc = accuracy/(dataloader.numOfUsers() - counter)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def training_loop_pop(args,\n",
        "                  model,\n",
        "                  tr_dataloader=None,\n",
        "                  validation=None,\n",
        "                  criterion_func=nn.MSELoss,\n",
        "                  ):\n",
        "    accuracy_by_epoch = []\n",
        "    criterion = criterion_func()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr,\n",
        "                           weight_decay=args.weight_decay)\n",
        "    popProb = tr_dataloader.popularity_prob\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "        model.train()\n",
        "        index = 0\n",
        "        for userVec in tqdm(tr_dataloader):\n",
        "            userItems = tr_dataloader.userSeenItems(index)\n",
        "            userPopProb = popProb.copy()*args.popularity_multiplyer\n",
        "            for item in userItems:\n",
        "                userPopProb[item] = 1\n",
        "            rand_vec = rand(args.input_size)\n",
        "            mask = np.zeros(args.input_size)\n",
        "            for j in range(args.input_size):\n",
        "                if(rand_vec[j] < userPopProb[j]):\n",
        "                    mask[j] = 1\n",
        "            userVec = torch.tensor(userVec).float()\n",
        "            # ===================forward=====================\n",
        "            output = model(userVec)\n",
        "            new_output = output*torch.tensor(mask)\n",
        "            loss = criterion(new_output, userVec.double())\n",
        "            # ===================backward====================\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            index += 1\n",
        "        # ===================log========================\n",
        "        if epoch % 1 == 0:\n",
        "            currentAccuracy = infer(\n",
        "                tr_dataloader, validation, model)\n",
        "            accuracy_by_epoch.append(currentAccuracy)\n",
        "            print(\n",
        "                f\" epoch: { epoch+1} tr_loss: {loss} validation accuracy: {currentAccuracy} \")\n",
        "            if(epoch % 8 == 0 and epoch != 0):\n",
        "                args.lr = args.lr*0.1\n",
        "        if currentAccuracy > 0.87:\n",
        "            break\n",
        "    predict = pd.read_csv(\"PopularityTest.csv\")\n",
        "    data = predict.values\n",
        "    for x in data:\n",
        "        user = x[0]-1\n",
        "        item1 = x[1]-1\n",
        "        item2 = x[2]-1\n",
        "        uservector = tr_dataloader.__getitem__(user)\n",
        "        uservector = torch.tensor(uservector).float()\n",
        "        with torch.no_grad():\n",
        "            output = model(uservector)\n",
        "            if output[item1] >= output[item2]:\n",
        "                x[3] = 0\n",
        "            if output[item1] < output[item2]:\n",
        "                x[3] = 1\n",
        "    df = pd.DataFrame(\n",
        "        data, columns=['UserID', 'Item1', 'Item2', 'bitClassification'])\n",
        "    df.to_csv(r'popularity_205592652_312425036.csv', index=False)\n",
        "    return accuracy_by_epoch\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ko2i7wH-eGN"
      },
      "source": [
        "    args.weight_decay  = 1e-7\n",
        "    args.hidden_size = 80\n",
        "    args.popularity_multiplyer = 150\n",
        "    model = AutoencoderPopular(args=args)\n",
        "    training_loop_pop(args,\n",
        "            model,\n",
        "            tr_dataloader=train_dataloader,\n",
        "            validation=validation_data,\n",
        "            criterion_func=nn.MSELoss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6040 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rand' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-24-a0cb7051818b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtr_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         criterion_func=nn.MSELoss)\n\u001b[0m",
            "\u001b[1;32m<ipython-input-23-65456db5405c>\u001b[0m in \u001b[0;36mtraining_loop_pop\u001b[1;34m(args, model, tr_dataloader, validation, criterion_func)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muserItems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0muserPopProb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mrand_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'rand' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}